{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = (15, 10) # use bigger graphs\n",
    "\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Reshape, Conv1D, MaxPooling1D, BatchNormalization, LeakyReLU\n",
    "from keras.layers import LSTM\n",
    "from keras import regularizers\n",
    "\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Added L2 penalty\n",
    "- Added extra features\n",
    "- Added strategy computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the file we processed in the second notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '/data/processed/cooked_v3.pkl'\n",
    "df = pd.read_pickle(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking what stocks are available in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['SNH', 'DBK', 'EOAN', 'DTE', 'CBK', 'RWE', 'IFX', 'SVAB', 'LHA',\n",
       "        'DAI', 'O2D', 'TKA', 'DPW', 'HDD', 'SIE', 'AIXA', 'BAYN', 'SAP',\n",
       "        'BAS', 'EVT', 'AT1', 'PSM', 'BMW', 'VOW3', 'FRE', 'GAZ', 'SDF',\n",
       "        'CEC', 'ALV', 'VNA', 'B4B', 'SHA', 'AB1', 'UN01', 'DLG', 'NOA3',\n",
       "        'NDX1', 'IGY', 'VODI', 'ADS', '1COV', 'TUI1', 'BPE5', 'HEI', 'KCO',\n",
       "        'ADV', 'SZU', 'EVK', 'HEN3', 'WDI', 'MUV2', 'DWNI', 'MRK', 'USE',\n",
       "        'PAH3', 'DEZ', 'FME', 'G1A', 'FNTN', 'RKET', 'QIA', 'DB1', 'ZAL',\n",
       "        'QSC', 'CON', 'SGL', 'BVB', 'TINA', 'PBB', 'PNE3', 'RIB', 'OSR',\n",
       "        'SHL', 'BEI', 'AOX', 'TEG', 'UTDI', 'ARL', 'MDG1', 'KGX', 'LXS',\n",
       "        'ARO', 'SANT', 'TTI', 'GYC', 'ANO', 'LINU', 'SOW', 'SZG', 'BOSS',\n",
       "        'LLD', 'BNR', 'WAF', 'LIN', 'DRI', 'NDA', 'ZIL2', 'SY1', 'CAP',\n",
       "        '3W9K'], dtype=object), 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mnemonics = df.Mnemonic.unique()\n",
    "df.Mnemonic.unique(), df.Mnemonic.unique().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what dates are available in the file. We will split the original set\n",
    "into three parts, train, valid, test based on the dates.\n",
    "If the dates are ordered chronologically, we take the first dates for the test set,\n",
    "then we take the next dates for the validation set and finally we take what is\n",
    "left for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201, ['2017-07-03', '2017-07-04'], ['2018-04-27', '2018-04-30'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def date_part(dt):\n",
    "    return str(dt).split(' ')[0]\n",
    "unique_days = sorted(list(set(map(date_part , list(df.index.unique())))))\n",
    "len(unique_days), unique_days[0:2], unique_days[-3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_train = 90.0\n",
    "percent_valid = 5\n",
    "percent_test = 100.0 - percent_train - percent_valid\n",
    "\n",
    "offset_train = int(len(unique_days)*percent_train/100.0)\n",
    "offset_test = offset_train + int(len(unique_days)*percent_valid/100.0)\n",
    "train_days = set(unique_days[0:offset_train])\n",
    "valid_days = set(unique_days[offset_train:offset_test])\n",
    "test_days = set(unique_days[offset_test:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CalcDateTime'] = df.index\n",
    "df['Date'] = df['CalcDateTime'].dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df.Date.isin(list(train_days))]\n",
    "df_valid = df[df.Date.isin(list(valid_days))]\n",
    "df_test = df[df.Date.isin(list(test_days))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've prepared the train, test and valid sets. Make sure the days do not overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CalcDateTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>129780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2017-08-09 08:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>2017-07-03 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>2018-03-20 20:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               CalcDateTime\n",
       "count              12978000\n",
       "unique               129780\n",
       "top     2017-08-09 08:10:00\n",
       "freq                    100\n",
       "first   2017-07-03 08:00:00\n",
       "last    2018-03-20 20:00:00"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[['CalcDateTime']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CalcDateTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>721000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2018-04-13 17:17:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>2018-03-21 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>2018-04-16 20:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               CalcDateTime\n",
       "count                721000\n",
       "unique                 7210\n",
       "top     2018-04-13 17:17:00\n",
       "freq                    100\n",
       "first   2018-03-21 08:00:00\n",
       "last    2018-04-16 20:00:00"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid[['CalcDateTime']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CalcDateTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>793100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2018-04-20 10:50:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>2018-04-17 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>2018-05-02 20:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               CalcDateTime\n",
       "count                793100\n",
       "unique                 7931\n",
       "top     2018-04-20 10:50:00\n",
       "freq                    100\n",
       "first   2018-04-17 08:00:00\n",
       "last    2018-05-02 20:00:00"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[['CalcDateTime']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the class below we create features from the raw features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped(ind, limit):\n",
    "    return np.where(ind < -limit, -limit, np.where(ind > limit, limit, ind ))\n",
    "\n",
    "class NARemover:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    def transform(self, single_stock):\n",
    "        before = single_stock.shape[0]\n",
    "        single_stock = single_stock.dropna()\n",
    "        after = single_stock.shape[0]\n",
    "        print(\"{}: Dropped {:2.2f} % of records due to NA\".format(self.name, 100.0*(before - after)/(0.0001 + before)))\n",
    "        return single_stock\n",
    "\n",
    "class Featurizer:\n",
    "    def transform(self, single_stock, max_offset = 10):\n",
    "        steps = 5    \n",
    "        limit = 5\n",
    "\n",
    "        # compute a smoothed standard deviation going back 5 and 10 steps\n",
    "        # used to be end_price\n",
    "        end_price_fixed = single_stock['EndPrice'].shift(1)\n",
    "        max_price_fixed = single_stock['MaxPrice'].shift(1)\n",
    "        min_price_fixed = single_stock['MinPrice'].shift(1)        \n",
    "\n",
    "        end_price_fixed = single_stock['EndPrice'].shift(1)\n",
    "        stds = 0.000001 + 0.9 * end_price_fixed.rolling(steps).std() + 0.1*end_price_fixed.rolling(10).std()\n",
    "\n",
    "        single_stock['x(NormalizingStd)'] = stds\n",
    "        \n",
    "        for offset in range(1, max_offset + 1):\n",
    "            # take the end price in the past at (t - offset)\n",
    "            end_price = single_stock['EndPrice'].shift(offset)\n",
    "\n",
    "            # take the max price in the past at (t - offset)        \n",
    "            min_price = single_stock['MinPrice'].shift(offset)\n",
    "\n",
    "            # take the min price in the past at (t - offset)\n",
    "            max_price = single_stock['MaxPrice'].shift(offset)\n",
    "        \n",
    "            # compute an indiator for time (t - offset)\n",
    "            ind = ((max_price - end_price) - (end_price - min_price)) / stds\n",
    "            single_stock['x((MaxP-EndP)-(EndP-MinP))(t - {})'.format(str(offset))] = clipped(ind, limit)   \n",
    "        \n",
    "            ind = (max_price - end_price) / stds\n",
    "            single_stock['x(MaxP - EndP)(t - {})'.format(str(offset))] = clipped(ind, limit)       \n",
    "        \n",
    "            ind = (end_price - min_price) / stds\n",
    "            single_stock['x(EndP-MinP)(t - {})'.format(str(offset))] = clipped(ind, limit)  \n",
    "                \n",
    "            ind = (end_price - end_price_fixed) / stds\n",
    "            single_stock['x(EndP-EndPBase)(t - {})'.format(str(offset))] = clipped(ind, limit)  \n",
    "                \n",
    "            ind = (max_price - max_price_fixed) / stds\n",
    "            single_stock['x(MaxP-MaxPBase)(t - {})'.format(str(offset))] = clipped(ind, limit) \n",
    "              \n",
    "            ind = (min_price - min_price_fixed) / stds\n",
    "            single_stock['x(MinP-MinPBase)(t - {})'.format(str(offset))] = clipped(ind, limit)              \n",
    "        \n",
    "        end_price_0 = single_stock['EndPrice'].shift(1)\n",
    "        for offset in range(2, max_offset):\n",
    "            end_price_1 = single_stock['EndPrice'].shift(offset + 1)             \n",
    "            ind = (end_price_0 - end_price_1) / stds\n",
    "            single_stock['x(EndP)(t - {}, t - {})'.format(str(1), str(offset + 1))] = \\\n",
    "                clipped(ind, limit)  \n",
    "             \n",
    "        for offset in range(2, max_offset):\n",
    "            end_price_1 = single_stock['EndPrice'].shift(offset + 1)             \n",
    "            single_stock['x(CombinedP)(t - {}, t - {})'.format(str(1), str(offset + 1))] = \\\n",
    "                single_stock['x(EndP)(t - {}, t - {})'.format(str(1), str(offset + 1))] * \\\n",
    "                single_stock['x((MaxP-EndP)-(EndP-MinP))(t - {})'.format(str(offset))]\n",
    "              \n",
    "        for offset in range(1, max_offset):\n",
    "            # take the end price in the past at (t - offset)\n",
    "            end_price = single_stock['EndPrice'].shift(offset)\n",
    "            end_price_1 = single_stock['EndPrice'].shift(offset + 1)  \n",
    "\n",
    "            # take the max price in the past at (t - offset)        \n",
    "            min_price = single_stock['MinPrice'].shift(offset)\n",
    "            min_price_1 = single_stock['MinPrice'].shift(offset + 1)\n",
    "\n",
    "            # take the min price in the past at (t - offset)\n",
    "            max_price = single_stock['MaxPrice'].shift(offset)\n",
    "            max_price_1 = single_stock['MaxPrice'].shift(offset + 1)            \n",
    "            \n",
    "            ind = (max_price - end_price_1) - (end_price_1 - min_price) / stds\n",
    "            single_stock['x((MaxP-EndP[-1])-(EndP-MinP[-1]))(t - {})'.format(str(offset))] = clipped(ind, limit)  \n",
    "            \n",
    "            ind = (max_price - end_price_1) - (max_price - end_price_1) / stds\n",
    "            single_stock['x(f1)(t - {})'.format(str(offset))] = clipped(ind, limit)  \n",
    "\n",
    "            ind = (min_price - end_price_1) - (min_price - end_price_1) / stds\n",
    "            single_stock['x(f2)(t - {})'.format(str(offset))] = clipped(ind, limit)\n",
    "            \n",
    "           \n",
    "            ind = (max_price - end_price) - (max_price_1 - end_price) / stds\n",
    "            single_stock['x(f3)(t - {})'.format(str(offset))] = clipped(ind, limit)  \n",
    "\n",
    "            ind = (min_price - end_price) - (min_price_1 - end_price) / stds\n",
    "            single_stock['x(f4)(t - {})'.format(str(offset))] = clipped(ind, limit)\n",
    "            \n",
    "        single_stock['y(Return)'] = (single_stock['EndPrice'] - single_stock['EndPrice'].shift(1))\n",
    "        ret = single_stock['y(Return)']/stds\n",
    "        single_stock['pseudo_y(ClippedReturn)'] = clipped(ret, limit) \n",
    "        single_stock['pseudo_y(SignReturn)'] = np.sign(single_stock['y(Return)'])\n",
    "        \n",
    "        # for evaluation we should use pct change\n",
    "        prev = single_stock['EndPrice'].shift(1)\n",
    "        single_stock['pseudo_y(pctChange)'] = (single_stock['EndPrice'] - prev)/prev\n",
    "        return single_stock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable we use for predictions start with `x(`, while the variables that should be predicted start with `y(`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mnemonic</th>\n",
       "      <th>MinPrice</th>\n",
       "      <th>MaxPrice</th>\n",
       "      <th>StartPrice</th>\n",
       "      <th>EndPrice</th>\n",
       "      <th>HasTrade</th>\n",
       "      <th>TradedVolume</th>\n",
       "      <th>NumberOfTrades</th>\n",
       "      <th>CalcDateTime</th>\n",
       "      <th>Date</th>\n",
       "      <th>...</th>\n",
       "      <th>x(f4)(t - 8)</th>\n",
       "      <th>x((MaxP-EndP[-1])-(EndP-MinP[-1]))(t - 9)</th>\n",
       "      <th>x(f1)(t - 9)</th>\n",
       "      <th>x(f2)(t - 9)</th>\n",
       "      <th>x(f3)(t - 9)</th>\n",
       "      <th>x(f4)(t - 9)</th>\n",
       "      <th>y(Return)</th>\n",
       "      <th>pseudo_y(ClippedReturn)</th>\n",
       "      <th>pseudo_y(SignReturn)</th>\n",
       "      <th>pseudo_y(pctChange)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CalcDateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-07-03 09:51:00</th>\n",
       "      <td>BMW</td>\n",
       "      <td>82.34</td>\n",
       "      <td>82.36</td>\n",
       "      <td>82.34</td>\n",
       "      <td>82.34</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2163.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2017-07-03 09:51:00</td>\n",
       "      <td>2017-07-03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.987222</td>\n",
       "      <td>-3.051665</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.021665</td>\n",
       "      <td>-3.031665</td>\n",
       "      <td>-1.027222</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.017222</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 136 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mnemonic  MinPrice  MaxPrice  StartPrice  EndPrice  \\\n",
       "CalcDateTime                                                             \n",
       "2017-07-03 09:51:00      BMW     82.34     82.36       82.34     82.34   \n",
       "\n",
       "                     HasTrade  TradedVolume  NumberOfTrades  \\\n",
       "CalcDateTime                                                  \n",
       "2017-07-03 09:51:00       1.0        2163.0            30.0   \n",
       "\n",
       "                           CalcDateTime        Date         ...           \\\n",
       "CalcDateTime                                                ...            \n",
       "2017-07-03 09:51:00 2017-07-03 09:51:00  2017-07-03         ...            \n",
       "\n",
       "                     x(f4)(t - 8)  x((MaxP-EndP[-1])-(EndP-MinP[-1]))(t - 9)  \\\n",
       "CalcDateTime                                                                   \n",
       "2017-07-03 09:51:00      0.987222                                  -3.051665   \n",
       "\n",
       "                     x(f1)(t - 9)  x(f2)(t - 9)  x(f3)(t - 9)  x(f4)(t - 9)  \\\n",
       "CalcDateTime                                                                  \n",
       "2017-07-03 09:51:00           0.0      3.021665     -3.031665     -1.027222   \n",
       "\n",
       "                     y(Return)  pseudo_y(ClippedReturn)  pseudo_y(SignReturn)  \\\n",
       "CalcDateTime                                                                    \n",
       "2017-07-03 09:51:00       0.01                 1.017222                   1.0   \n",
       "\n",
       "                     pseudo_y(pctChange)  \n",
       "CalcDateTime                              \n",
       "2017-07-03 09:51:00             0.000121  \n",
       "\n",
       "[1 rows x 136 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = df_train[df_train.Mnemonic == 'BMW'].copy()\n",
    "dummy = dummy[dummy.HasTrade == 1.0].iloc[100:111]\n",
    "Featurizer().transform(dummy)\n",
    "old = dummy.iloc[[-1]].copy()\n",
    "old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mnemonic</th>\n",
       "      <th>MinPrice</th>\n",
       "      <th>MaxPrice</th>\n",
       "      <th>StartPrice</th>\n",
       "      <th>EndPrice</th>\n",
       "      <th>HasTrade</th>\n",
       "      <th>TradedVolume</th>\n",
       "      <th>NumberOfTrades</th>\n",
       "      <th>CalcDateTime</th>\n",
       "      <th>Date</th>\n",
       "      <th>...</th>\n",
       "      <th>x(f4)(t - 8)</th>\n",
       "      <th>x((MaxP-EndP[-1])-(EndP-MinP[-1]))(t - 9)</th>\n",
       "      <th>x(f1)(t - 9)</th>\n",
       "      <th>x(f2)(t - 9)</th>\n",
       "      <th>x(f3)(t - 9)</th>\n",
       "      <th>x(f4)(t - 9)</th>\n",
       "      <th>y(Return)</th>\n",
       "      <th>pseudo_y(ClippedReturn)</th>\n",
       "      <th>pseudo_y(SignReturn)</th>\n",
       "      <th>pseudo_y(pctChange)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CalcDateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-07-03 09:51:00</th>\n",
       "      <td>BMW</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2163.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2017-07-03 09:51:00</td>\n",
       "      <td>2017-07-03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.987222</td>\n",
       "      <td>-3.051665</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.021665</td>\n",
       "      <td>-3.031665</td>\n",
       "      <td>-1.027222</td>\n",
       "      <td>-82.33</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 136 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mnemonic  MinPrice  MaxPrice  StartPrice  EndPrice  \\\n",
       "CalcDateTime                                                             \n",
       "2017-07-03 09:51:00      BMW       0.0       0.0       82.34       0.0   \n",
       "\n",
       "                     HasTrade  TradedVolume  NumberOfTrades  \\\n",
       "CalcDateTime                                                  \n",
       "2017-07-03 09:51:00       1.0        2163.0            30.0   \n",
       "\n",
       "                           CalcDateTime        Date         ...           \\\n",
       "CalcDateTime                                                ...            \n",
       "2017-07-03 09:51:00 2017-07-03 09:51:00  2017-07-03         ...            \n",
       "\n",
       "                     x(f4)(t - 8)  x((MaxP-EndP[-1])-(EndP-MinP[-1]))(t - 9)  \\\n",
       "CalcDateTime                                                                   \n",
       "2017-07-03 09:51:00      0.987222                                  -3.051665   \n",
       "\n",
       "                     x(f1)(t - 9)  x(f2)(t - 9)  x(f3)(t - 9)  x(f4)(t - 9)  \\\n",
       "CalcDateTime                                                                  \n",
       "2017-07-03 09:51:00           0.0      3.021665     -3.031665     -1.027222   \n",
       "\n",
       "                     y(Return)  pseudo_y(ClippedReturn)  pseudo_y(SignReturn)  \\\n",
       "CalcDateTime                                                                    \n",
       "2017-07-03 09:51:00     -82.33                     -5.0                  -1.0   \n",
       "\n",
       "                     pseudo_y(pctChange)  \n",
       "CalcDateTime                              \n",
       "2017-07-03 09:51:00                 -1.0  \n",
       "\n",
       "[1 rows x 136 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.ix[ -1, 'MinPrice'] = 0.0\n",
    "dummy.ix[ -1, 'MaxPrice'] = 0.0\n",
    "dummy.ix[ -1, 'EndPrice'] = 0.0\n",
    "\n",
    "\n",
    "Featurizer().transform(dummy)\n",
    "dummy.iloc[[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure old features and new features computed with different values of EndPrice, MaxPrice, MinPrice of the last\n",
    "record give the same values for the `x(` features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in dummy.dtypes.index:\n",
    "    if f.startswith('x('):\n",
    "        assert dummy[f][-1] == old[f][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingSet:\n",
    "    def __init__(self, X, y, orig_df):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.original_df = orig_df\n",
    "        \n",
    "class TrainingSetBuilder:\n",
    "    def transform(self, single_stock):\n",
    "        x_features = filter(lambda name: name.startswith('x('), list(single_stock.dtypes.index))\n",
    "        X = single_stock[x_features].values\n",
    "        y = single_stock[['pseudo_y(SignReturn)']].values\n",
    "        return TrainingSet(X, y, single_stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictions:\n",
    "    def __init__(self, predictions, training_set):\n",
    "\n",
    "        self.predictions = predictions\n",
    "        self.training_set = training_set\n",
    "        \n",
    "    def evaluate(self):\n",
    "        single_feature = 'x((MaxP-EndP)-(EndP-MinP))(t - 1)'\n",
    "        stats_df = pd.DataFrame({\n",
    "                      'predictions': self.predictions[:,0],\n",
    "                      'single_feature_pred': self.training_set.original_df[single_feature].values,\n",
    "                      'pseudo_y(SignReturn)': self.training_set.y[:,0],\n",
    "                      'pseudo_y(pctChange)': self.training_set.original_df['pseudo_y(pctChange)'].values,\n",
    "                       'pseudo_y(ClippedReturn)': self.training_set.original_df['pseudo_y(ClippedReturn)'].values,\n",
    "                      'y(Return)': self.training_set.original_df['y(Return)'].values})\n",
    "        \n",
    "        corr = stats_df. \\\n",
    "            corr()[['predictions', 'single_feature_pred']]. \\\n",
    "            iloc[1:]\n",
    "            \n",
    "        pred_signs = np.sign(stats_df['predictions'])\n",
    "        y_signs = np.sign(stats_df['y(Return)'])\n",
    "        has_answer = np.absolute(pred_signs * y_signs).sum()\n",
    "        correct = np.where(pred_signs * y_signs == 1.0, 1.0, 0.0).sum()\n",
    "        \n",
    "        thresholds = []\n",
    "        accuracy = []\n",
    "        correct_lst = []\n",
    "        errors = []\n",
    "        percent_has_answer = []\n",
    "        abs_has_answer = []\n",
    "        achieved_returns = []\n",
    "\n",
    "        preds = stats_df['predictions']\n",
    "        \n",
    "        for d in range(5, 46, 5):\n",
    "            low = np.percentile(preds, d) \n",
    "            high = np.percentile(preds, 100 - d)\n",
    "            thresholded = np.where(preds > high, 1.0, np.where(preds < low, -1.0, 0.0))\n",
    "            c = np.where(np.sign(thresholded)*np.sign(y_signs) == 1.0, 1.0, 0.0).sum()\n",
    "            e = np.where(np.sign(thresholded)*np.sign(y_signs) == -1.0, 1.0, 0.0).sum()\n",
    "            achieved_ret = (stats_df['pseudo_y(pctChange)']*thresholded).sum()\n",
    "            #print(\"threshold\", d, \"accuracy\", c/(c + e), \"abs count\",  c + e, \"no answer\", pred_signs.shape[0]  - c - e\n",
    "            correct_lst.append(c)\n",
    "            errors.append(e)\n",
    "            accuracy.append(c/(c + e))\n",
    "            percent_has_answer.append(100.0*(c + e)/pred_signs.shape[0])\n",
    "            abs_has_answer.append((c + e))\n",
    "            achieved_returns.append(achieved_ret)\n",
    "            thresholds.append(d)\n",
    "            \n",
    "        at_cutoff = DataFrame({\n",
    "                    'thresholds': thresholds,\n",
    "                    'accuracy': accuracy,\n",
    "                    'percent_with_answer': percent_has_answer,\n",
    "                    'absolute_has_answer': abs_has_answer,\n",
    "                    'achieved_returns': achieved_returns,\n",
    "                    'correct': correct_lst,\n",
    "                    'errors': errors\n",
    "        })\n",
    "        at_cutoff['achieved_norm_returns'] = at_cutoff['achieved_returns']/at_cutoff['absolute_has_answer']\n",
    "        \n",
    "        ret = stats_df['pseudo_y(pctChange)']\n",
    "        rand_feature = np.where(np.random.rand(ret.shape[0]) > 0.5, 1.0, -1.0)    \n",
    "        random_returns = (ret * rand_feature).sum()\n",
    "        always_up_returns = (ret*1.0).sum()\n",
    "        always_down_returns = (ret*-1.0).sum()\n",
    "        omnicient_returns = (np.absolute(ret)).sum()\n",
    "        achieved = (ret * pred_signs).sum()\n",
    "        return {\n",
    "            'corr': corr,\n",
    "            'accuracy_at_cutoff': at_cutoff,\n",
    "            'matches': {\n",
    "                'percent_correct': 100*correct/has_answer,\n",
    "                'percent_has_answer': has_answer/pred_signs.shape[0],\n",
    "                'absolute_with_answer': has_answer,\n",
    "                'size': pred_signs.shape[0]\n",
    "            },\n",
    "            'strategies': {\n",
    "                'omniscient': omnicient_returns,\n",
    "                'random': random_returns,\n",
    "                'always_up': always_up_returns,\n",
    "                'always_down': always_down_returns,\n",
    "                'achieved': achieved,\n",
    "                'num_trials': np.absolute(pred_signs).sum()\n",
    "            }\n",
    "        }\n",
    "    \n",
    "class MLModel:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        \n",
    "    def fit(self, training_set, valid_set = None):\n",
    "        train_X, train_y = training_set.X, training_set.y\n",
    "        \n",
    "        if valid_set is None:\n",
    "            valid_X, valid_y = train_X, train_y\n",
    "        else:\n",
    "            valid_X, valid_y = valid_set.X, valid_set.y\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        #model.add(Dense(10, activation='relu', input_shape =(train_X.shape[1],)))    \n",
    "        #model.add(Dense(10, activation='relu'))\n",
    "        #model.add(Dense(1))\n",
    "        model.add(Dense(100, activation='relu', input_shape =(train_X.shape[1],),\n",
    "                        kernel_regularizer=regularizers.l2(0.001))) \n",
    "        model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.001)))        \n",
    "        #model.add(Dense(10, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        self.model = model            \n",
    "\n",
    "        # fit network\n",
    "        # change the epochs back to 20\n",
    "        history = model.fit(train_X, train_y, epochs=50, batch_size=2500, validation_data=(valid_X, valid_y), verbose=2, shuffle=True)\n",
    "        # plot history\n",
    "        pyplot.plot(history.history['loss'], label='train')\n",
    "        pyplot.plot(history.history['val_loss'], label='test')\n",
    "        pyplot.legend()\n",
    "        pyplot.show()\n",
    "        \n",
    "    def transform(self, input_set):\n",
    "        predictions = self.model.predict(input_set.X)\n",
    "        return Predictions(predictions, input_set)\n",
    "    \n",
    "    def fit_transform(self, training_set, valid_set):\n",
    "        self.fit(training_set, valid_set)\n",
    "        return self.transform(training_set), self.transform(valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNH: Dropped 0.01 % of records due to NA\n",
      "(67791, 136)\n",
      "SNH: Dropped 0.26 % of records due to NA\n",
      "SNH: Dropped 0.29 % of records due to NA\n",
      "DBK: Dropped 0.01 % of records due to NA\n",
      "(84215, 136)\n",
      "DBK: Dropped 0.22 % of records due to NA\n",
      "DBK: Dropped 0.21 % of records due to NA\n",
      "EOAN: Dropped 0.01 % of records due to NA\n",
      "(84125, 136)\n",
      "EOAN: Dropped 0.22 % of records due to NA\n",
      "EOAN: Dropped 0.21 % of records due to NA\n",
      "DTE: Dropped 0.01 % of records due to NA\n",
      "(80688, 136)\n",
      "DTE: Dropped 0.22 % of records due to NA\n",
      "DTE: Dropped 0.22 % of records due to NA\n",
      "CBK: Dropped 0.01 % of records due to NA\n",
      "(81420, 136)\n",
      "CBK: Dropped 0.22 % of records due to NA\n",
      "CBK: Dropped 0.22 % of records due to NA\n",
      "RWE: Dropped 0.01 % of records due to NA\n",
      "(80890, 136)\n",
      "RWE: Dropped 0.23 % of records due to NA\n",
      "RWE: Dropped 0.23 % of records due to NA\n",
      "IFX: Dropped 0.01 % of records due to NA\n",
      "(80542, 136)\n",
      "IFX: Dropped 0.23 % of records due to NA\n",
      "IFX: Dropped 0.23 % of records due to NA\n",
      "SVAB: Dropped 0.13 % of records due to NA\n",
      "(7776, 136)\n",
      "SVAB: Dropped 3.03 % of records due to NA\n",
      "SVAB: Dropped 3.42 % of records due to NA\n",
      "LHA: Dropped 0.01 % of records due to NA\n",
      "(80945, 136)\n",
      "LHA: Dropped 0.22 % of records due to NA\n",
      "LHA: Dropped 0.22 % of records due to NA\n",
      "DAI: Dropped 0.01 % of records due to NA\n",
      "(84506, 136)\n",
      "DAI: Dropped 0.22 % of records due to NA\n",
      "DAI: Dropped 0.21 % of records due to NA\n",
      "O2D: Dropped 0.02 % of records due to NA\n",
      "(56567, 136)\n",
      "O2D: Dropped 0.31 % of records due to NA\n",
      "O2D: Dropped 0.27 % of records due to NA\n",
      "TKA: Dropped 0.01 % of records due to NA\n",
      "(80096, 136)\n",
      "TKA: Dropped 0.24 % of records due to NA\n",
      "TKA: Dropped 0.24 % of records due to NA\n",
      "DPW: Dropped 0.01 % of records due to NA\n",
      "(81247, 136)\n",
      "DPW: Dropped 0.22 % of records due to NA\n",
      "DPW: Dropped 0.23 % of records due to NA\n",
      "HDD: Dropped 0.03 % of records due to NA\n",
      "(38340, 136)\n",
      "HDD: Dropped 0.53 % of records due to NA\n",
      "HDD: Dropped 0.71 % of records due to NA\n",
      "SIE: Dropped 0.01 % of records due to NA\n",
      "(80194, 136)\n",
      "SIE: Dropped 0.22 % of records due to NA\n",
      "SIE: Dropped 0.21 % of records due to NA\n",
      "AIXA: Dropped 0.02 % of records due to NA\n",
      "(56187, 136)\n",
      "AIXA: Dropped 0.33 % of records due to NA\n",
      "AIXA: Dropped 0.29 % of records due to NA\n",
      "BAYN: Dropped 0.01 % of records due to NA\n",
      "(78212, 136)\n",
      "BAYN: Dropped 0.21 % of records due to NA\n",
      "BAYN: Dropped 0.21 % of records due to NA\n",
      "SAP: Dropped 0.01 % of records due to NA\n",
      "(84169, 136)\n",
      "SAP: Dropped 0.22 % of records due to NA\n",
      "SAP: Dropped 0.21 % of records due to NA\n",
      "BAS: Dropped 0.01 % of records due to NA\n",
      "(84554, 136)\n",
      "BAS: Dropped 0.21 % of records due to NA\n",
      "BAS: Dropped 0.21 % of records due to NA\n",
      "EVT: Dropped 0.02 % of records due to NA\n",
      "(62188, 136)\n",
      "EVT: Dropped 0.37 % of records due to NA\n",
      "EVT: Dropped 0.37 % of records due to NA\n",
      "AT1: Dropped 0.03 % of records due to NA\n",
      "(33179, 136)\n",
      "AT1: Dropped 0.38 % of records due to NA\n",
      "AT1: Dropped 0.47 % of records due to NA\n",
      "PSM: Dropped 0.01 % of records due to NA\n",
      "(77231, 136)\n",
      "PSM: Dropped 0.31 % of records due to NA\n",
      "PSM: Dropped 0.33 % of records due to NA\n",
      "BMW: Dropped 0.01 % of records due to NA\n",
      "(83967, 136)\n",
      "BMW: Dropped 0.22 % of records due to NA\n",
      "BMW: Dropped 0.21 % of records due to NA\n",
      "VOW3: Dropped 0.01 % of records due to NA\n",
      "(81363, 136)\n",
      "VOW3: Dropped 0.22 % of records due to NA\n",
      "VOW3: Dropped 0.21 % of records due to NA\n",
      "FRE: Dropped 0.01 % of records due to NA\n",
      "(81925, 136)\n",
      "FRE: Dropped 0.23 % of records due to NA\n",
      "FRE: Dropped 0.22 % of records due to NA\n",
      "GAZ: Dropped 0.04 % of records due to NA\n",
      "(23606, 136)\n",
      "GAZ: Dropped 0.64 % of records due to NA\n",
      "GAZ: Dropped 1.09 % of records due to NA\n",
      "SDF: Dropped 0.02 % of records due to NA\n",
      "(61711, 136)\n",
      "SDF: Dropped 0.33 % of records due to NA\n",
      "SDF: Dropped 0.37 % of records due to NA\n",
      "CEC: Dropped 0.02 % of records due to NA\n",
      "(47371, 136)\n",
      "CEC: Dropped 0.35 % of records due to NA\n",
      "CEC: Dropped 0.39 % of records due to NA\n",
      "ALV: Dropped 0.01 % of records due to NA\n",
      "(81778, 136)\n",
      "ALV: Dropped 0.22 % of records due to NA\n",
      "ALV: Dropped 0.21 % of records due to NA\n",
      "VNA: Dropped 0.01 % of records due to NA\n",
      "(77788, 136)\n",
      "VNA: Dropped 0.24 % of records due to NA\n",
      "VNA: Dropped 0.24 % of records due to NA\n",
      "B4B: Dropped 0.02 % of records due to NA\n",
      "(50860, 136)\n",
      "B4B: Dropped 0.35 % of records due to NA\n",
      "B4B: Dropped 0.32 % of records due to NA\n",
      "SHA: Dropped 0.02 % of records due to NA\n",
      "(56879, 136)\n",
      "SHA: Dropped 0.33 % of records due to NA\n",
      "SHA: Dropped 0.33 % of records due to NA\n",
      "AB1: Dropped 0.12 % of records due to NA\n",
      "(8462, 136)\n",
      "AB1: Dropped 22.73 % of records due to NA\n",
      "AB1: Dropped 27.78 % of records due to NA\n",
      "UN01: Dropped 0.02 % of records due to NA\n",
      "(62411, 136)\n",
      "UN01: Dropped 0.38 % of records due to NA\n",
      "UN01: Dropped 0.34 % of records due to NA\n",
      "DLG: Dropped 0.02 % of records due to NA\n",
      "(63254, 136)\n",
      "DLG: Dropped 0.33 % of records due to NA\n",
      "DLG: Dropped 0.31 % of records due to NA\n",
      "NOA3: Dropped 0.04 % of records due to NA\n",
      "(22535, 136)\n",
      "NOA3: Dropped 0.99 % of records due to NA\n",
      "NOA3: Dropped 0.65 % of records due to NA\n",
      "NDX1: Dropped 0.02 % of records due to NA\n",
      "(40692, 136)\n",
      "NDX1: Dropped 0.54 % of records due to NA\n",
      "NDX1: Dropped 0.70 % of records due to NA\n",
      "IGY: Dropped 0.02 % of records due to NA\n",
      "(66419, 136)\n",
      "IGY: Dropped 0.28 % of records due to NA\n",
      "IGY: Dropped 0.28 % of records due to NA\n",
      "VODI: Dropped 0.07 % of records due to NA\n",
      "(14756, 136)\n",
      "VODI: Dropped 0.98 % of records due to NA\n",
      "VODI: Dropped 1.24 % of records due to NA\n"
     ]
    }
   ],
   "source": [
    "combined_training_set = []\n",
    "combined_valid_set = []\n",
    "combined_test_set = []\n",
    "\n",
    "for mnemonic in all_mnemonics: #['SIE', 'BMW', 'SNH', 'DBK', 'EOAN',  'DTE', 'CBK', 'RWE', 'IFX']:\n",
    "\n",
    "    single_stock = df_train[df_train.Mnemonic == mnemonic].copy()\n",
    "    single_stock = single_stock[single_stock.HasTrade == 1.0]\n",
    "    single_stock = Featurizer().transform(single_stock)\n",
    "    single_stock = NARemover(mnemonic).transform(single_stock)\n",
    "    combined_training_set.append(single_stock)\n",
    "    print(single_stock.shape)\n",
    "\n",
    "    single_stock = df_valid[df_valid.Mnemonic == mnemonic].copy()\n",
    "    single_stock = single_stock[single_stock.HasTrade == 1.0] \n",
    "    single_stock = Featurizer().transform(single_stock)\n",
    "    single_stock = NARemover(mnemonic).transform(single_stock)\n",
    "    combined_valid_set.append(single_stock)\n",
    "    \n",
    "    single_stock = df_test[df_test.Mnemonic == mnemonic].copy()\n",
    "    single_stock = single_stock[single_stock.HasTrade == 1.0] \n",
    "    single_stock = Featurizer().transform(single_stock)\n",
    "    single_stock = NARemover(mnemonic).transform(single_stock)\n",
    "    combined_test_set.append(single_stock)\n",
    "    \n",
    "combined_training_set_df = pd.concat(combined_training_set, axis=0)\n",
    "training_set = TrainingSetBuilder().transform(combined_training_set_df)\n",
    "    \n",
    "combined_valid_set_df = pd.concat(combined_valid_set, axis=0)\n",
    "valid_set = TrainingSetBuilder().transform(combined_valid_set_df) \n",
    "\n",
    "combined_test_set_df = pd.concat(combined_test_set, axis=0)\n",
    "test_set = TrainingSetBuilder().transform(combined_test_set_df) \n",
    "    \n",
    "model = MLModel()\n",
    "train_predictions, valid_predictions = model.fit_transform(training_set, valid_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions.evaluate()['matches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_predictions.evaluate()['matches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.transform(test_set)\n",
    "test_predictions.evaluate()['matches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions.evaluate()['accuracy_at_cutoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_baseline(d):\n",
    "    single_feature = 'x((MaxP-EndP)-(EndP-MinP))(t - 1)'\n",
    "    preds = d.training_set.original_df[single_feature].values\n",
    "    preds = preds.reshape((preds.shape[0], 1))\n",
    "    return Predictions(preds, d.training_set).evaluate()\n",
    "\n",
    "def readable_summary(which_set, p):\n",
    "    achieved = p.evaluate()['strategies']['achieved']\n",
    "    achieved_baseline = pred_baseline(p)['strategies']['achieved']\n",
    "    per_change = np.mean(np.absolute(p.training_set.original_df['pseudo_y(pctChange)']))\n",
    "    n = p.training_set.original_df.shape[0]\n",
    "    print (\"\"\"So if you play {} times on the {} with 1 EUR and you always guess the movement,\n",
    "ignoring all transactions cost, you will make {}. \n",
    "Instead you make {} or {} percent of the ideally achievable.\n",
    "If you use the baseline you will make {} or {} percent of ideal\"\"\".format(\n",
    "        n, which_set, n * per_change, achieved, 100.0*achieved/(n*per_change),\n",
    "          achieved_baseline, 100.0*achieved_baseline/(n*per_change)))\n",
    "readable_summary('Training Set', train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions.evaluate()['strategies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_baseline(valid_predictions)['strategies'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_predictions.evaluate()['accuracy_at_cutoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for the baseline\n",
    "pred_baseline(test_predictions)['accuracy_at_cutoff'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for the baseline\n",
    "pred_baseline(test_predictions)['strategies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for ML\n",
    "test_predictions.evaluate()['accuracy_at_cutoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ML\n",
    "Predictions(test_predictions.predictions, test_predictions.training_set).evaluate()['strategies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions.evaluate()['accuracy_at_cutoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readable_summary('Test set', test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_predictions.training_set.original_df[['y(Return)']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingore stuff below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.sign(valid_predictions.predictions)\n",
    "# I'm adding signs on the returns\n",
    "returns = np.sign(valid_predictions.training_set.original_df['y(Return)'])\n",
    "returns = returns.reshape((returns.shape[0], 1))\n",
    "np.sum(np.multiply(p, returns))/np.sum(np.absolute(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = valid_predictions.predictions\n",
    "d = 5\n",
    "low = np.percentile(preds, d) \n",
    "high = np.percentile(preds, 100 - d)\n",
    "thresholded = np.where(preds > high, 1.0, np.where(preds < low, -1.0, 0.0))\n",
    "omniscient=np.sign(returns)\n",
    "omniscient=np.where(np.absolute(thresholded) > 0.0, omniscient,0.0)\n",
    "ours = np.sum(np.multiply(thresholded, returns)) #/np.sum(np.absolute(thresholded))\n",
    "oracle = np.sum(np.multiply(omniscient, returns)) # /np.sum(np.absolute(thresholded))\n",
    "ours, oracle, 100.0*ours/oracle, np.sum(np.absolute(thresholded)), np.sum(np.absolute(omniscient))\n",
    "np.min(np.multiply(thresholded, returns)), np.max(np.multiply(thresholded, returns))\n",
    "\n",
    "cs_ours = np.cumsum(np.multiply(thresholded, returns))\n",
    "cs_omniscient = np.cumsum(np.multiply(omniscient, returns))\n",
    "mydf = DataFrame({'ours': cs_ours, 'om': cs_omniscient})\n",
    "mydf.plot()\n",
    "ours, oracle, 100.0*ours/oracle, np.sum(np.absolute(thresholded)), np.sum(np.absolute(omniscient))\n",
    "cs_ours[-1], cs_omniscient[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_predictions.training_set.original_df['AbsReturn'] = np.absolute(valid_predictions.training_set.original_df['y(Return)'])\n",
    "r = valid_predictions.training_set.original_df.groupby('Mnemonic')[['AbsReturn']].mean()\n",
    "\n",
    "# TODO:\n",
    "r.index\n",
    "r['AbsReturn'].values\n",
    "#MULT IS 1.0/(0.00001 + mean(absReturn))\n",
    "# or look in the past k steps and compute the std. dev, and divide by the std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
